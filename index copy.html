<!doctype html>
<html lang="en">
    <head>
        <title>IR3D-Bench</title>
        <link rel="icon" type="image/x-icon" href="/static/img/icons/jellyfish.ico">

        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1">

        <!-- Open Graph -->
        <meta property="og:url" content="https://IR3D-Bench.github.io/" />
        <meta property="og:image" content="https://IR3D-Bench.github.io/static/img/IR3D_preview.png" />
        <meta property="og:title" content="IR3D-Bench: Evaluating Vision-Language Model Scene Understanding as Agentic Inverse Rendering." />
        <meta property="og:description" content="IR3D-Bench challenges vision-language models to demonstrate real scene understanding by recreating 3D structures from images using tools, not just describing them." />

        <!-- Twitter -->
        <meta name="twitter:url" content="https://IR3D-Bench.github.io/" />
        <meta name="twitter:card" content="summary_large_image" />
        <meta name="twitter:image" content="https://IR3D-Bench.github.io/static/img/IR3D_preview.png" />
        <meta name="twitter:title" content="IR3D-Bench: Evaluating Vision-Language Model Scene Understanding as Agentic Inverse Rendering." />
        <meta name="twitter:description" content="We introduce IR3D-Bench, a benchmark that challenges vision-language models to demonstrate real scene understanding by recreating 3D structures from images using tools, not just describing them." />

        <script src="./static/js/distill_template.v2.js"></script>
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
        <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

        <script src="https://d3js.org/d3.v5.min.js"></script>
        <script src="https://d3js.org/d3-collection.v1.min.js"></script>
        <script src="https://rawgit.com/nstrayer/slid3r/master/dist/slid3r.js"></script>

        <script defer="" src="./static/js/hider.js"></script>
        <script src="./static/js/image_interact.js"></script>
        <script src="./static/js/switch_videos.js"></script>

        <link rel="stylesheet" href="./static/css/style.css">
        <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
        <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">

        <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/katex.min.css" integrity="sha384-yFRtMMDnQtDRO8rLpMIKrtPCD5jdktao2TV19YiZYWMDkUR5GQZR/NOVTdquEx1j" crossorigin="anonymous">
        <script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/katex.min.js" integrity="sha384-9Nhn55MVVN0/4OFx7EE5kpFBPsEMZxKTCnA+4fqDmg12eCTqGi6+BB2LjY8brQxJ" crossorigin="anonymous"></script>
        <script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"
            onload="renderMathInElement(document.body);"></script>
        <script defer src="./static/js/fontawesome.all.min.js"></script>


        <!-- medium zoom https://github.com/francoischalifour/medium-zoom -->
        <script src="https://cdn.jsdelivr.net/npm/jquery@3.7.1/dist/jquery.min.js"></script>  <!-- jquery -->
        <script defer src="./static/js/medium-zoom.min.js"></script>
        <script defer src="./static/js/zoom.js"></script>
    </head>
   

    <body>
        <div class="header-wrapper">
            <div class="header-container" id="header-container">
                <div class="header-content">
                    <h1 style="margin-top: 0px">IR3D-Bench: Evaluating Vision-Language Model Scene Understanding as Agentic Inverse Rendering</h1>
                    <h2>
                        <br> 
                        <i>
                            "What I cannot create, I do not understand." <br>
                            <span style="display: block; text-align: right;">——Richard Feynman</span>
                        </i>
                        <br> 
                    </h2>
                    <div class="button-container">
                        <!-- replace arxiv -->
                        <a href="https://arxiv.org/abs/2406.16860" class="button paper-link" target="_blank">
                            <span class="icon is-small">
                                <i class="ai ai-arxiv "  style="height: 1.5em;"></i>
                            </span>
                            arXiv
                        </a>
                        <!-- replace pdf -->
                        <a href="https://arxiv.org/pdf/2406.16860" class="button paper-link" target="_blank">
                            <span class="icon is-small"  style="height: 1.5em;">
                                <i class="fas fa-file-pdf"></i>
                            </span>
                            <span>pdf</span>
                        </a>
                        <!-- replace image -->
                        <a href="https://github.com/cambrian-mllm/cambrian" class="button" target="_blank">
                            <span class="icon is-small"  style="height: 1.5em;">
                                <i class="fab fa-github"></i>
                            </span>
                            <span>Code</span>
                        </a>
                        <!-- <br> -->
                        <a href="https://huggingface.co/collections/nyu-visionx/cambrian-1-models-666fa7116d5420e514b0f23c" class="button" target="_blank">
                            <span class="icon is-small">
                                <img src="https://www.logo.wine/a/logo/YouTube/YouTube-Icon-Full-Color-Logo.wine.svg" alt="Hugging Face logo" style="height: 1.5em;">
                            </span>
                            <span>Video</span>
                        </a>
                        <a href="https://huggingface.co/collections/nyu-visionx/cambrian-data-6667ce801e179b4fbe774e11" class="button" target="_blank">
                            <span class="icon is-small">
                                <img src="https://huggingface.co/front/assets/huggingface_logo-noborder.svg" alt="Hugging Face logo" style="height: 1.5em;">
                            </span>
                            <span>Data</span>
                        </a>
                    </div>
                </div>
                <div class="header-image">
                    <img draggable="false" src="static/img/IR3D.png" alt="Teaser Image" class="teaser-image">
                </div>
            </div>
        </div>
    <d-article>
        <div class="byline">
            <div class="byline-container">
                <p>
                    <a href="https://liuhengyu321.github.io/" class="author-link" target="_blank">Hengyu Liu<sup>1,*</sup></a>,
                    <a href="https://chenxinli001.github.io/" class="author-link" target="_blank">Chenxin Li<sup>1,*</sup></a>,
                    <a href="https://github.com/Lizx123456" class="author-link" target="_blank">Zhengxin Li<sup>2</sup></a>,
                    <a href="https://github.com/wind-bell999" class="author-link" target="_blank">Yipeng Wu<sup>2</sup></a>,
                    <a href="https://wymancv.github.io/wuyang.github.io/" class="author-link" target="_blank">Wuyang Li<sup>3</sup></a>,
                    <a href="https://visitworld123.github.io/" class="author-link" target="_blank">Zhiqin Yang<sup>1</sup></a>,
                    <a href="https://openreview.net/profile?id=~Zhenyuan_Zhang2" class="author-link" target="_blank">Zhenyuan Zhang<sup>4</sup></a>,
                    <a href="https://lyl1015.github.io/" class="author-link" target="_blank">Yunlong Lin<sup>5</sup></a>,
                    <a href="https://facultyprofiles.hkust.edu.hk/profiles.php?profile=sirui-han-siruihan" class="author-link" target="_blank">Sirui Han<sup>4,&dagger;</sup></a>,
                    <a href="https://brandonyfeng.github.io/" class="author-link" target="_blank">Brandon Y. Feng<sup>6,&dagger;</sup></a>
                </p>
                <p>
                    <sup>1</sup>CUHK, <sup>2</sup>TJU, <sup>3</sup>EPFL, <sup>4</sup>HKUST, <sup>5</sup>XMU, <sup>6</sup>MIT
                </p>
                <p style="text-align: center; margin-bottom: 0;">
                    <span class="author-note"><sup>*</sup>Equal Contribution</span>&emsp;
                    <span class="author-note"><sup>&dagger;</sup>Corresponding author</span>
                </p>
            </div>
        </div>

        <div class="l-page video-container" style="margin-left: 4%; margin-bottom: 20px">
            <iframe width="560" height="315" src="https://www.youtube.com/embed/k6aDJUfxs4Q" title="YouTube video player" frameborder="0" allow="autoplay; encrypted-media; picture-in-picture" allowfullscreen></iframe>
        </div>

        <p class="text abstract">

            Vision-language models (VLMs) excel at descriptive tasks, but <strong><a href="#motivation">&sect;whether they truly understand scenes from visual observations remains uncertain</a></strong>.

            We introduce IR3D-Bench, a benchmark challenging VLMs to demonstrate understanding through active creation rather than passive recognition. 
            Grounded in the <strong><a href="#dataset_intergation">&sect;analysis-by-synthesis paradigm</a></strong>, 
            IR3D-Bench <strong><a href="#prompt">&sect;tasks Vision-Language Agents (VLAs)</a></strong>
            with actively using programming and rendering tools to recreate the underlying 3D structure of an input image, 
            achieving  <strong><a href="#inverse_rendering">&sect;agentic inverse rendering through tool use</a></strong>. 
            
            This "understanding-by-creating" approach probes the tool-using generative capacity of VLAs, 
            moving beyond the descriptive or conversational capacity measured by traditional scene understanding benchmarks. 

            We provide <strong><a href="#metrics">&sect;a comprehensive suite of metrics</a></strong> to evaluate geometric accuracy, spatial relations, appearance attributes, and overall plausibility. 

            <strong><a href="#exp">&sect;Initial experimental results</a></strong> on agentic inverse rendering powered by various state-of-the-art VLMs highlight current limitations, particularly in visual precision rather than basic tool usage.
        </p>

        <div class="icon-row">
            <a href="#dataset_intergation" class="icon-link">
                <img src="static/img/icons/recipe.svg" alt="Visual Representation Logo" class="icon">
                Dataset<br>Integration
            </a>
            <a href="#prompt" class="icon-link">
                <img src="static/img/icons/data.svg" alt="Connector Logo" class="icon">
                Task<br>Prompt
            </a>
            <a href="#inverse_rendering" class="icon-link">
                <img src="static/img/icons/visual.svg" alt="Data Logo" class="icon">
                Inverse<br>Rendering
            </a>
            <a href="#metrics" class="icon-link">
                <img src="static/img/icons/eval.svg" alt="Recipe Logo" class="icon">
                Benchmark<br>Metrics
            </a>
        </div>

        <p class="click-hint" style="width: 85%;">
            <img src="static/img/icons/click.gif" style="width: 1.5rem">
            <strong>Click to jump to each section.</strong>
        </p>
        <hr>
        <div id='visual_representations' class="vision-block">

            <div id="motivation" class="sub-section">
                <h1 class="text">Motivation</h1>

                    <d-figure id="fig-comparison" >
                        <figure>
                            <img data-zoomable="" draggable="false" src="static/img/teaser.png" alt="benchmark category">
                        </figure>
                    </d-figure>
                    <p class="text">
                        <p class="text">
                        Humans demonstrate true understanding through creation and recreate observed scenes because we genuinely comprehend spatial relationships and physical attributes. 
                        In contrast, current Vision-Language Agents (VLAs) are primarily evaluated on recognition tasks like captioning or QA, which fail to assess deeper understanding. 
                        <strong>Can VLAs truly understand what they see ?</strong> IR3D-Bench test it by letting them recreating the observations.
                        </p>
                    </p>
            </div>
            
            <div id="stage1" class="sub-section">

            <h1 class="text">Stage 1: Dataset Integration and Inverse Rendering </h1>
                <d-figure id="fig-studyadapter" >
                    <figure>
                        <img data-zoomable="" draggable="false" src="static/img/stage1.png" alt="Stage 1 Pipeline">
                    </figure>
                </d-figure>
                
            </div>

            <div id='dataset_intergation' class="viusal-representation-block">
            <h2 class="text">CLEVR Dataset Integration</h2>

                <p class="text">
                    We use the CLEVR dataset, a popular benchmark for 3D vision tasks. Our work uses the validation split, which includes 15,000 synthetic images. Each image contains 3 to 10 objects with detailed annotations covering their 3D coordinates, pixel-space projections, shape, color, size, material, and spatial relationships. These rich annotations make CLEVR ideal for evaluating 3D reconstruction and spatial reasoning in a controlled environment.
                </p>

                <div class="text" style="grid-column: text;">
                    <!-- Image Carousel Section -->
                    <div class="image-carousel-container">
                        <div class="image-carousel">
                            <div class="carousel-item" data-json-src="static/img/dataset/CLEVR_val_000000.json">
                                <img src="static/img/dataset/CLEVR_val_000000.png" alt="CLEVR Dataset Sample 1">
                                <script type="application/json">
                                    {"info":{"split":"val","license":"Creative Commons Attribution (CC BY 4.0)","version":"1.0","date":"2/14/2017"},"documentation":{},"scenes":[{"image_index":0,"objects":[{"color":"brown","size":"large","rotation":178.92387258999463,"shape":"cylinder","3d_coords":[-1.4937210083007812,-1.9936031103134155,0.699999988079071],"material":"rubber","pixel_coords":[119,131,10.801968574523926]},{"color":"gray","size":"large","rotation":243.405459279722,"shape":"cube","3d_coords":[1.555708646774292,-2.104736566543579,0.699999988079071],"material":"rubber","pixel_coords":[198,190,8.60103988647461]},{"color":"green","size":"small","rotation":230.45235024165092,"shape":"cylinder","3d_coords":[-2.342184543609619,-0.5205014944076538,0.3499999940395355],"material":"rubber","pixel_coords":[161,118,12.372727394104004]},{"color":"purple","size":"large","rotation":31.654351858799153,"shape":"sphere","3d_coords":[-0.8073106408119202,1.914123773574829,0.699999988079071],"material":"metal","pixel_coords":[282,100,12.495001792907715]},{"color":"gray","size":"small","rotation":42.183287560575,"shape":"cube","3d_coords":[2.6763813495635986,0.03453871235251427,0.3499999940395355],"material":"metal","pixel_coords":[337,195,9.161211967468262]}],"relationships":{"right":[[1,2,3,4],[3,4],[1,3,4],[4],[]],"behind":[[2,3],[0,2,3,4],[3],[],[0,2,3]],"front":[[1,4],[],[0,1,4],[0,1,2,4],[1]],"left":[[],[0,2],[0],[0,1,2],[0,1,2,3]]},"image_filename":"CLEVR_val_000000.png","split":"val","directions":{"right":[0.6563112735748291,0.7544902563095093,-0.0],"behind":[-0.754490315914154,0.6563112735748291,0.0],"above":[0.0,0.0,1.0],"below":[-0.0,-0.0,-1.0],"left":[-0.6563112735748291,-0.7544902563095093,0.0],"front":[0.754490315914154,-0.6563112735748291,-0.0]}}]}
                                </script>
                            </div>
                            
                            <div class="carousel-item" data-json-src="static/img/dataset/CLEVR_val_000001.json">
                                <img src="static/img/dataset/CLEVR_val_000001.png" alt="CLEVR Dataset Sample 2">
                                <script type="application/json">
                                    {"info":{"split":"val","license":"Creative Commons Attribution (CC BY 4.0)","version":"1.0","date":"2/14/2017"},"documentation":{},"scenes":[{"image_index":1,"objects":[{"color":"blue","size":"small","rotation":87.2092786731497,"shape":"cylinder","3d_coords":[-1.4019312858581543,0.9176712036132812,0.3499999940395355],"material":"metal","pixel_coords":[227,114,12.651700019836426]},{"color":"blue","size":"large","rotation":196.10032228152653,"shape":"cylinder","3d_coords":[2.56838321685791,2.8128368854522705,0.699999988079071],"material":"rubber","pixel_coords":[419,128,11.159161567687988]},{"color":"purple","size":"large","rotation":335.9272462668898,"shape":"sphere","3d_coords":[2.9524991512298584,0.7795983552932739,0.699999988079071],"material":"metal","pixel_coords":[382,168,9.672847747802734]},{"color":"yellow","size":"small","rotation":148.95236071940306,"shape":"cube","3d_coords":[-0.6843340992927551,2.7821667194366455,0.3499999940395355],"material":"metal","pixel_coords":[299,102,13.345067024230957]},{"color":"gray","size":"large","rotation":80.12030414245824,"shape":"cylinder","3d_coords":[-2.4355783462524414,-2.236754894256592,0.699999988079071],"material":"metal","pixel_coords":[85,127,11.196852684020996]},{"color":"cyan","size":"small","rotation":331.3705215814606,"shape":"cube","3d_coords":[-2.969482421875,1.423396110534668,0.3499999940395355],"material":"metal","pixel_coords":[200,92,13.94270133972168]},{"color":"cyan","size":"small","rotation":285.3619516996765,"shape":"cylinder","3d_coords":[2.6561522483825684,-1.5428955554962158,0.3499999940395355],"material":"rubber","pixel_coords":[286,229,8.604046821594238]},{"color":"blue","size":"large","rotation":284.89181152858265,"shape":"cube","3d_coords":[1.3208903074264526,0.4931657910346985,0.699999988079071],"material":"metal","pixel_coords":[304,144,10.518533706665039]},{"color":"green","size":"small","rotation":179.1419631256681,"shape":"cylinder","3d_coords":[-0.4053439795970917,-2.4375929832458496,0.3499999940395355],"material":"metal","pixel_coords":[133,180,9.971955299377441]},{"color":"yellow","size":"small","rotation":334.56764345600396,"shape":"cylinder","3d_coords":[0.9365460872650146,-2.5308165550231934,0.3499999940395355],"material":"rubber","pixel_coords":[173,210,9.074959754943848]}],"relationships":{"right":[[1,2,3,6,7],[],[1],[1,2],[0,1,2,3,5,6,7,8,9],[0,1,2,3,6,7],[1,2,3,7],[1,2,3],[0,1,2,3,5,6,7,9],[0,1,2,3,5,6,7]],"behind":[[1,3,5],[0,1,2,3,5,6,7,8,9],[1,3,5],[1,5],[],[1],[0,1,2,3,5,7,8,9],[1,3,5],[0,1,2,3,5,6,7],[0,1,2,3,5,6,7,8]],"front":[[2,4,6,7,8,9],[],[0,4,6,7,8,9],[0,2,4,6,7,8,9],[0,1,2,3,5,6,7,8,9],[2,4,6,7,8,9],[4,8,9],[],[],[]],"left":[[],[0,2,3,4,5,6,7,8,9],[0,3,4,5,8,9],[0,4,5,8,9],[0,2,3,5,6,7,8,9],[0,3,4],[0,2,3,4,5,8,9],[0,2,3,4,5,6,8,9],[0,2,3,4,5],[0,2,3,4,5,8]]},"image_filename":"CLEVR_val_000001.png","split":"val","directions":{"right":[0.6563112735748291,0.7544902563095093,-0.0],"behind":[-0.754490315914154,0.6563112735748291,0.0],"above":[0.0,0.0,1.0],"below":[-0.0,-0.0,-1.0],"left":[-0.6563112735748291,-0.7544902563095093,0.0],"front":[0.754490315914154,-0.6563112735748291,-0.0]}}]}
                                </script>
                            </div>
                            
                            <div class="carousel-item" data-json-src="static/img/dataset/CLEVR_val_000002.json">
                                <img src="static/img/dataset/CLEVR_val_000002.png" alt="CLEVR Dataset Sample 3">
                                <script type="application/json">
                                    {"info":{"split":"val","license":"Creative Commons Attribution (CC BY 4.0)","version":"1.0","date":"2/14/2017"},"documentation":{},"scenes":[{"image_index":2,"objects":[{"color":"purple","size":"large","rotation":24.266859933294032,"shape":"cube","3d_coords":[0.9164416790008545,2.5542008876800537,0.699999988079071],"material":"metal","pixel_coords":[349,113,12.158169746398926]},{"color":"yellow","size":"large","rotation":126.90972419473805,"shape":"sphere","3d_coords":[-2.3131093978881836,1.4229497909545898,0.699999988079071],"material":"metal","pixel_coords":[223,86,13.650497436523438]},{"color":"blue","size":"small","rotation":156.39099006528448,"shape":"cube","3d_coords":[-2.4592032432556152,-2.859107732772827,0.3499999940395355],"material":"metal","pixel_coords":[67,146,11.473008155822754]},{"color":"green","size":"small","rotation":42.165222443059704,"shape":"sphere","3d_coords":[1.6640125513076782,-0.6807796955108643,0.3499999940395355],"material":"metal","pixel_coords":[270,187,9.986722946166992]},{"color":"purple","size":"large","rotation":76.43620097602489,"shape":"cylinder","3d_coords":[2.9267563819885254,1.7387181520462036,0.699999988079071],"material":"metal","pixel_coords":[404,156,10.361809730529785]},{"color":"brown","size":"small","rotation":346.39110722841224,"shape":"cylinder","3d_coords":[-2.6752941608428955,-0.15827248990535736,0.3499999940395355],"material":"metal","pixel_coords":[166,111,13.157706260681152]},{"color":"gray","size":"large","rotation":271.4579996980617,"shape":"cylinder","3d_coords":[2.776763439178467,-2.3596839904785156,0.699999988079071],"material":"rubber","pixel_coords":[242,235,8.121735572814941]},{"color":"yellow","size":"large","rotation":26.23470743173072,"shape":"cube","3d_coords":[0.945395290851593,-2.2262401580810547,0.699999988079071],"material":"rubber","pixel_coords":[180,185,9.410319328308105]}],"relationships":{"right":[[4],[0,3,4,6],[0,1,3,4,5,6,7],[0,4],[0,3,4],[0,1,2,3,4,6,7],[0,3,4],[0,1,2,3,4,6]],"behind":[[1,5],[0,2,3,4,5,6,7],[0,3],[],[0,1,2,3,5,6],[0,1,2,3,6],[0,2,3]],"front":[[1,2,4,5,6],[4,5],[1,4,5,6],[0,1,2,4,5,6],[],[4],[1,4,5]],"left":[[1,2,4,5],[2,4,5],[],[0,1,2,4,5,6],[0,1,2,5,6],[0,1,2,3,4,6],[0,1,2,3,4,5]]},"image_filename":"CLEVR_val_000002.png","split":"val","directions":{"right":[0.6563112735748291,0.7544902563095093,-0.0],"behind":[-0.754490315914154,0.6563112735748291,0.0],"above":[0.0,0.0,1.0],"below":[-0.0,-0.0,-1.0],"left":[-0.6563112735748291,-0.7544902563095093,0.0],"front":[0.754490315914154,-0.6563112735748291,-0.0]}}]}
                                </script>
                            </div>
                            
                            <div class="carousel-item" data-json-src="static/img/dataset/CLEVR_val_000003.json">
                                <img src="static/img/dataset/CLEVR_val_000003.png" alt="CLEVR Dataset Sample 4">
                                <script type="application/json">
                                    {"info":{"split":"val","license":"Creative Commons Attribution (CC BY 4.0)","version":"1.0","date":"2/14/2017"},"documentation":{},"scenes":[{"image_index":3,"objects":[{"color":"yellow","size":"small","rotation":52.53684703422393,"shape":"sphere","3d_coords":[2.1417222023010254,-2.7576637268066406,0.3499999940395355],"material":"rubber","pixel_coords":[197,247,8.038165092468262]},{"color":"purple","size":"large","rotation":277.821313290429,"shape":"sphere","3d_coords":[-2.072401523590088,2.136496067047119,0.699999988079071],"material":"metal","pixel_coords":[249,81,13.48475170135498]},{"color":"blue","size":"large","rotation":77.97216669037716,"shape":"cylinder","3d_coords":[-0.528790295124054,2.905188798904419,0.699999988079071],"material":"metal","pixel_coords":[315,90,12.911911010742188]},{"color":"gray","size":"large","rotation":277.43883851053334,"shape":"cube","3d_coords":[-0.9423105716705322,-2.95906138420105,0.699999988079071],"material":"rubber","pixel_coords":[87,159,9.787914276123047]},{"color":"brown","size":"small","rotation":215.92210360473527,"shape":"cylinder","3d_coords":[-0.9988107085227966,-1.528560996055603,0.3499999940395355],"material":"metal","pixel_coords":[152,151,10.821432113647461]},{"color":"purple","size":"large","rotation":214.7023250953585,"shape":"sphere","3d_coords":[0.4423714578151703,1.3085707426071167,0.699999988079071],"material":"metal","pixel_coords":[299,120,11.346577644348145]},{"color":"brown","size":"small","rotation":31.799854282664057,"shape":"cylinder","3d_coords":[-1.8510726690292358,-0.1959560215473175,0.3499999940395355],"material":"rubber","pixel_coords":[181,121,12.15542221069336]}],"relationships":{"right":[[1,2,5],[2,5],[],[0,1,2,4,5,6],[0,1,2,5,6],[2],[0,1,2,5]],"behind":[[3],[0,2,3,6],[0,3],[],[0,1,2,3,5,6],[0,1,2,3,6],[0,2,3]],"front":[[1,2,4,5,6],[4,5],[1,4,5,6],[0,1,2,4,5,6],[],[4],[1,4,5]],"left":[[1,2,4,5],[2,4,5],[],[0,1,2,4,5,6],[0,1,2,5,6],[0,1,2,3,4,6],[0,1,2,3,4,5]]},"image_filename":"CLEVR_val_000003.png","split":"val","directions":{"right":[0.6563112735748291,0.7544902563095093,-0.0],"behind":[-0.754490315914154,0.6563112735748291,0.0],"above":[0.0,0.0,1.0],"below":[-0.0,-0.0,-1.0],"left":[-0.6563112735748291,-0.7544902563095093,0.0],"front":[0.754490315914154,-0.6563112735748291,-0.0]}}]}
                                </script>
                            </div>
                            
                            <div class="carousel-item" data-json-src="static/img/dataset/CLEVR_val_000004.json">
                                <img src="static/img/dataset/CLEVR_val_000004.png" alt="CLEVR Dataset Sample 5">
                                <script type="application/json">
                                    {"info":{"split":"val","license":"Creative Commons Attribution (CC BY 4.0)","version":"1.0","date":"2/14/2017"},"documentation":{},"scenes":[{"image_index":4,"objects":[{"color":"red","size":"small","rotation":103.96810996862779,"shape":"sphere","3d_coords":[-1.3702292442321777,-2.155432939529419,0.3499999940395355],"material":"rubber","pixel_coords":[110,154,10.136273384094238]},{"color":"brown","size":"large","rotation":300.285267307425,"shape":"sphere","3d_coords":[0.46740391850471497,-2.6708884239196777,0.699999988079071],"material":"rubber","pixel_coords":[137,183,8.460342407226562]},{"color":"purple","size":"small","rotation":123.9522959520619,"shape":"cylinder","3d_coords":[-1.6677716970443726,1.5341471433639526,0.3499999940395355],"material":"rubber","pixel_coords":[241,103,12.526554107666016]},{"color":"gray","size":"large","rotation":27.441103922672664,"shape":"cylinder","3d_coords":[2.624936103820801,1.3213320970535278,0.699999988079071],"material":"rubber","pixel_coords":[393,153,9.417881965637207]},{"color":"gray","size":"large","rotation":277.7327613525135,"shape":"sphere","3d_coords":[0.3835722506046295,2.3025729656219482,0.699999988079071],"material":"metal","pixel_coords":[330,105,11.473976135253906]},{"color":"green","size":"large","rotation":62.00128204401709,"shape":"cylinder","3d_coords":[-2.771306276321411,-2.472614049911499,0.699999988079071],"material":"rubber","pixel_coords":[59,121,10.705843925476074]}],"relationships":{"right":[[1,2,3,4],[2,3,4],[3,4],[],[3],[0,1,2,3,4]],"behind":[[2,4,5],[0,2,3,4,5],[],[0,2,4,5],[2],[2,4]],"front":[[1,3],[],[0,1,3,4,5],[1],[0,1,3,5],[0,1,3]],"left":[[5],[0,5],[0,1,5],[0,1,2,4,5],[0,1,2,5],[]]},"image_filename":"CLEVR_val_000004.png","split":"val","directions":{"right":[0.6563112735748291,0.7544902563095093,-0.0],"behind":[-0.754490315914154,0.6563112735748291,0.0],"above":[0.0,0.0,1.0],"below":[-0.0,-0.0,-1.0],"left":[-0.6563112735748291,-0.7544902563095093,0.0],"front":[0.754490315914154,-0.6563112735748291,-0.0]}}]}
                                </script>
                            </div>
                            
                            <div class="carousel-item" data-json-src="static/img/dataset/CLEVR_val_000005.json">
                                <img src="static/img/dataset/CLEVR_val_000005.png" alt="CLEVR Dataset Sample 6">
                                <script type="application/json">
                                    {"info":{"split":"val","license":"Creative Commons Attribution (CC BY 4.0)","version":"1.0","date":"2/14/2017"},"documentation":{},"scenes":[{"image_index":5,"objects":[{"color":"yellow","size":"small","rotation":312.44380611548513,"shape":"cube","3d_coords":[-0.6884685754776001,1.4769357442855835,0.3499999940395355],"material":"rubber","pixel_coords":[268,119,12.629888534545898]},{"color":"gray","size":"large","rotation":301.9143106752324,"shape":"sphere","3d_coords":[-0.4024123251438141,-1.1227504014968872,0.699999988079071],"material":"metal","pixel_coords":[186,139,10.785597801208496]},{"color":"blue","size":"small","rotation":147.11529758732306,"shape":"cube","3d_coords":[-1.9388166666030884,-0.8367786407470703,0.3499999940395355],"material":"metal","pixel_coords":[158,128,12.150248527526855]},{"color":"yellow","size":"large","rotation":306.2918951162928,"shape":"sphere","3d_coords":[-1.4938887357711792,2.907089948654175,0.699999988079071],"material":"metal","pixel_coords":[288,83,13.830824851989746]},{"color":"blue","size":"small","rotation":210.8307452645328,"shape":"cube","3d_coords":[1.7929556369781494,-2.2758824825286865,0.3499999940395355],"material":"metal","pixel_coords":[206,220,8.803750991821289]},{"color":"yellow","size":"large","rotation":73.44739856943892,"shape":"sphere","3d_coords":[-0.7093875408172607,-2.8273890018463135,0.699999988079071],"material":"rubber","pixel_coords":[103,159,10.017038345336914]},{"color":"yellow","size":"large","rotation":344.38317584188644,"shape":"sphere","3d_coords":[1.2460074424743652,2.012741804122925,0.699999988079071],"material":"metal","pixel_coords":[347,124,11.466705322265625]}],"relationships":{"right":[[3,6],[0,3,4,6],[0,1,3,4,6],[6],[0,3,6],[0,1,2,3,4,6],[]],"behind":[[3],[0,2,3,6],[0,3],[],[0,1,2,3,5,6],[0,1,2,3,6],[0,2,3]],"front":[[1,2,4,5,6],[4,5],[1,4,5,6],[0,1,2,4,5,6],[],[4],[1,4,5]],"left":[[1,2,4,5],[2,4,5],[],[0,1,2,4,5,6],[0,1,2,5,6],[0,1,2,3,4,6],[0,1,2,3,4,5]]},"image_filename":"CLEVR_val_000005.png","split":"val","directions":{"right":[0.6563112735748291,0.7544902563095093,-0.0],"behind":[-0.754490315914154,0.6563112735748291,0.0],"above":[0.0,0.0,1.0],"below":[-0.0,-0.0,-1.0],"left":[-0.6563112735748291,-0.7544902563095093,0.0],"front":[0.754490315914154,-0.6563112735748291,-0.0]}}]}
                                </script>
                            </div>
                        </div>
                    </div>

                    <!-- Text Display Area -->
                    <div class="text-display-area">
                        <div class="text-container">
                            <p id="display-text">Click the image to view the GT annotations</p>
                        </div>
                    </div>
                </div>

                <style>
                    /* Image Carousel Styles */
                    .image-carousel-container {
                        position: relative;
                        margin: 3rem 0 0;
                        padding-bottom: 0;
                        overflow: hidden;
                        width: 100%;
                    }
                    
                    @keyframes slideCarousel {
                        to {
                           transform: translate(calc(-50% - 0.25rem)); /* 0.25rem is half the gap */
                        }
                    }
                    
                    .image-carousel {
                        display: flex;
                        width: max-content;
                        animation: slideCarousel 20s linear infinite;
                        gap: 0.5rem;
                    }
                    
                    .image-carousel:hover {
                        animation-play-state: paused;
                    }
                    
                    .carousel-item {
                        flex: 0 0 400px;
                        min-width: 300px;
                        max-width: 400px;
                        scroll-snap-align: start;
                        border-radius: 8px;
                        overflow: hidden;
                        box-shadow: 0 4px 12px rgba(0, 0, 0, 0.05);
                        background-color: white;
                        transition: transform 0.3s ease, box-shadow 0.3s ease;
                        position: relative;
                        margin-right: 0.5rem;
                        cursor: pointer;
                    }
                    
                    .carousel-item:hover {
                        transform: translateY(-5px);
                        box-shadow: 0 8px 16px rgba(0, 0, 0, 0.1);
                    }
                    
                    .carousel-item img {
                        width: 100%;
                        height: 250px;
                        object-fit: cover;
                        display: block;
                    }
                    
                    .hidden-text-data {
                        display: none;
                    }
                    
                    /* Text Display Area Styles */
                    .text-display-area {
                        background-color: #ffefdc; /* Imitate color from image */
                        border-radius: 12px;
                        padding: 1.5rem;
                        margin-top: 1rem; /* This now controls the distance */
                        box-shadow: 0 4px 12px rgba(0, 0, 0, 0.05);
                        min-height: 80px;
                        display: flex;
                        align-items: center;
                    }
                    
                    .text-container {
                        max-width: 100%;
                        margin: 0 auto;
                        width: 100%;
                    }
                    
                    .text-container p {
                        font-size: 1rem;
                        line-height: 1.5;
                        color: #333;
                        margin: 0;
                        text-align: left;
                        transition: opacity 0.3s ease;
                        white-space: pre-wrap;
                        font-family: monospace;
                    }
                    
                    /* Responsive adjustments */
                    @media (max-width: 768px) {
                        .carousel-item {
                            flex: 0 0 90%;
                            min-width: 250px;
                            margin-right: 0.25rem;
                        }
                        
                        .image-carousel {
                            animation-duration: 30s; /* Adjust mobile speed */
                        }
                    }
                </style>

                <script>
                    // Image Carousel functionality
                    document.addEventListener('DOMContentLoaded', function() {
                        const carouselContainer = document.querySelector('.image-carousel-container');
                        const carousel = document.querySelector('.image-carousel');
                        const displayText = document.getElementById('display-text');
                        
                        // Duplicate all carousel items for infinite loop
                        const items = Array.from(carousel.children);
                        items.forEach(item => {
                            const clone = item.cloneNode(true);
                            clone.setAttribute('aria-hidden', 'true');
                            carousel.appendChild(clone);
                        });

                        // Adjust animation speed based on screen width
                        function updateCarouselSpeed() {
                            const isMobile = window.innerWidth <= 768;
                            carousel.style.animationDuration = isMobile ? '30s' : '20s';
                        }
                        
                        // Initial setup
                        updateCarouselSpeed();
                        
                        // Update on window resize
                        window.addEventListener('resize', updateCarouselSpeed);
                        
                        // Pause animation on hover
                        carousel.addEventListener('mouseenter', () => {
                            carousel.style.animationPlayState = 'paused';
                        });
                        
                        carousel.addEventListener('mouseleave', () => {
                            carousel.style.animationPlayState = 'running';
                            displayText.style.opacity = '0';
                            setTimeout(() => {
                                displayText.textContent = 'Click the image to view the GT annotations';
                                displayText.style.opacity = '1';
                            }, 150);
                        });
                        
                        // Update text on hover
                        const allItems = carousel.querySelectorAll('.carousel-item');
                        allItems.forEach(item => {
                            item.addEventListener('mouseenter', function() {
                                updateText(this);
                            });
                        });
                        
                        // Function to update the text display by fetching JSON
                        function updateText(item) {
                            const jsonScript = item.querySelector('script[type="application/json"]');
                            if (!jsonScript) {
                                displayText.textContent = 'No JSON data found for this item.';
                                return;
                            }

                            displayText.style.opacity = '0';

                            try {
                                const data = JSON.parse(jsonScript.textContent);

                                if (data.scenes && data.scenes[0] && data.scenes[0].objects && data.scenes[0].objects[0]) {
                                    const firstObject = data.scenes[0].objects[0];
                                    const textContent = JSON.stringify(firstObject, null, 2) + '...';
                                    
                                    setTimeout(() => {
                                        displayText.textContent = textContent;
                                        displayText.style.opacity = '1';
                                    }, 150);
                                } else {
                                    setTimeout(() => {
                                        displayText.textContent = 'Could not find valid object data in the JSON file.';
                                        displayText.style.opacity = '1';
                                    }, 150);
                                }
                            } catch (error) {
                                console.error('Error parsing JSON:', error);
                                setTimeout(() => {
                                    displayText.textContent = 'Error parsing object details.';
                                    displayText.style.opacity = '1';
                                }, 150);
                            }
                        }
                        
                        // No need to initialize with first item's text anymore
                    });
                </script>

                <p class="text">
                    <strong>Narrowing the gap between CLIP and SSL models</strong>
                    Above, we observe that DINOv2 stands midway between SSL models and CLIP models on general VQA and knowledge VQA tasks,
                    even outperforming some CLIP models on vision-centric benchmarks with higher resolution.
                    We investigate unfreezing the vision backbones and increasing the amount of visual fine-tuning data to narrow this gap.
                    In <a href="#fig-narrowgap">Figure 5</a>, we observe that by unfreezing the vision backbone,
                    the DINOv2-based MLLM fine-tuned with 5M data surpasses the MLLM trained with a CLIP model on 0.7M data.
                    Additionally, the gap between DINOv2 and the CLIP models is reduced under the 5M data experiment setting.
                </p>
                <d-figure id="fig-narrowgap">
                    <figure>
                        <img data-zoomable="" draggable="false" src="static/img/narrow_gap.png" alt="Narrowing the gap between CLIP and SSL models">
                        <figcaption>
                            <strong>Figure 5:</strong> By unfreezing the visual backbone and fine-tuning on 5M examples, the gap between CLIP and DINOv2 can be narrowed.
                        </figcaption>
                    </figure>
                </d-figure>

                <p class="text">
                    <strong>Combining Multiple Vision Encoders </strong>
                    As observed in <a href="#fig-mllm_as_interface">Figure 4</a>, different vision models excel in different aspects of MLLM performance.
                    We explore the potential of combining multiple vision encoders to leverage their distinctive representations.
                    Given that different vision encoders use varying architectures and image resolutions, we interpolate the output visual tokens to a fixed number, 576.
                    The results are tabulated in <a href="#tab:model_ensemble">Table 2</a>, where we observe consistent performance improvements with the addition of more models.
                </p>

                
                <div id="tab:model_ensemble" style="display: flex; flex-direction: column; align-items: center;">
                    <div class="table-container">
                        <table class="data-table">
                            <thead>
                            <tr>
                                <th colspan="1" class="tb-hdr">Vision Backbone</th>
                                <th colspan="1" class="tb-hdr"></th>
                                <th colspan="4" class="tb-hdr">General</th>
                                <th colspan="4" class="tb-hdr">Knowledge</th>
                                <th colspan="4" class="tb-hdr">OCR & Chart</th>
                                <th colspan="4" class="tb-hdr">Vision-Centric</th>
                            </tr>
                            <tr>
                                <th class="section-border">Encoders</th>
                                <th class="section-border"><b>Average</b></th>
                                <th>MME<sup>P</sup></th>
                                <th>MMB</th>
                                <th>SEED<sup>I</sup></th>
                                <th class="section-border">GQA</th>
                                <th>SQA<sup>I</sup></th>
                                <th>MMMU<sup>V</sup></th>
                                <th>MathVista<sup>M</sup></th>
                                <th class="section-border">AI2D</th>
                                <th>ChartQA</th>
                                <th>OCRBench</th>
                                <th>TextVQA</th>
                                <th class="section-border">DocVQA</th>
                                <th>MMVP</th>
                                <th>RealWorldQA</th>
                                <th>CV-Bench<sup>2D</sup></th>
                                <th>CV-Bench<sup>3D</sup></th>
                            </tr>
                            </thead>
                            <tbody>
                            <tr>
                                <td class="section-border">SigLIP+DINOv2</td>
                                <td class="section-border">51.61</td>
                                <td>1,432.02</td>
                                <td>61.28</td>
                                <td>65.99</td>
                                <td class="section-border">63.30</td>
                                <td>68.82</td>
                                <td>35.69</td>
                                <td>29.40</td>
                                <td class="section-border">60.01</td>
                                <td>43.00</td>
                                <td>35.70</td>
                                <td>60.40</td>
                                <td class="section-border">37.54</td>
                                <td>30.00</td>
                                <td>53.99</td>
                                <td>55.52</td>
                                <td>53.58</td>
                            </tr>
                            <tr>
                                <td class="section-border">SigLIP+DINOv2+ConvNext</td>
                                <td class="section-border">54.52</td>
                                <td>1,503.51</td>
                                <td>63.83</td>
                                <td>67.97</td>
                                <td class="section-border">63.95</td>
                                <td>70.40</td>
                                <td class="highlight">35.99</td>
                                <td>29.30</td>
                                <td class="section-border">60.69</td>
                                <td>48.20</td>
                                <td>36.90</td>
                                <td>64.97</td>
                                <td class="section-border">45.53</td>
                                <td class="highlight">34.67</td>
                                <td>58.69</td>
                                <td>55.74</td>
                                <td>60.33</td>
                            </tr>
                            <tr>
                                <td class="section-border">SigLIP+DINOv2+ConvNext+CLIP</td>
                                <td class="section-border highlight">54.74</td>
                                <td>1,479.46</td>
                                <td>63.32</td>
                                <td>67.63</td>
                                <td class="section-border highlight">64.04</td>
                                <td class="highlight">71.39</td>
                                <td>35.49</td>
                                <td>29.10</td>
                                <td class="section-border">59.88</td>
                                <td>50.24</td>
                                <td class="highlight">39.60</td>
                                <td>64.55</td>
                                <td class="section-border">46.12</td>
                                <td>32.67</td>
                                <td class="highlight">58.95</td>
                                <td>58.54</td>
                                <td class="highlight">60.42</td>
                            </tr>
                            <tr>
                                <td class="section-border">SigLIP+ConvNext</td>
                                <td class="section-border">54.53</td>
                                <td>1,494.97</td>
                                <td class="highlight">64.60</td>
                                <td>67.98</td>
                                <td class="section-border">63.58</td>
                                <td>71.05</td>
                                <td>34.90</td>
                                <td>29.80</td>
                                <td class="section-border">60.85</td>
                                <td>50.64</td>
                                <td>38.00</td>
                                <td>64.53</td>
                                <td class="section-border">46.52</td>
                                <td>32.00</td>
                                <td>57.91</td>
                                <td class="highlight">58.83</td>
                                <td>56.58</td>
                            </tr>
                            <tr>
                                <td class="section-border">CLIP+ConvNext</td>
                                <td class="section-border">54.45</td>
                                <td class="highlight">1,511.08</td>
                                <td>63.83</td>
                                <td>67.41</td>
                                <td class="section-border">63.63</td>
                                <td>70.80</td>
                                <td>35.09</td>
                                <td>30.40</td>
                                <td class="section-border">59.91</td>
                                <td>51.32</td>
                                <td>35.00</td>
                                <td>64.45</td>
                                <td class="section-border">47.88</td>
                                <td>33.33</td>
                                <td>57.25</td>
                                <td>56.32</td>
                                <td>59.08</td>
                            </tr>
                            <tr>
                                <td class="section-border">SigLIP+DINOv2+ConvNext</td>
                                <td class="section-border">53.78</td>
                                <td>1,450.64</td>
                                <td>63.57</td>
                                <td>67.79</td>
                                <td class="section-border">63.63</td>
                                <td>71.34</td>
                                <td>34.80</td>
                                <td>30.20</td>
                                <td class="section-border highlight">61.04</td>
                                <td>49.32</td>
                                <td>37.70</td>
                                <td>64.05</td>
                                <td class="section-border">45.83</td>
                                <td>30.00</td>
                                <td>56.21</td>
                                <td>58.08</td>
                                <td>54.33</td>
                            </tr>
                            <tr>
                                <td class="section-border">SigLIP+CLIP+ConvNext</td>
                                <td class="section-border">54.53</td>
                                <td>1,507.28</td>
                                <td>63.23</td>
                                <td class="highlight">68.64</td>
                                <td class="section-border">63.63</td>
                                <td>71.10</td>
                                <td>35.89</td>
                                <td class="highlight">30.90</td>
                                <td class="section-border">59.97</td>
                                <td class="highlight">52.36</td>
                                <td>38.50</td>
                                <td class="highlight">65.40</td>
                                <td class="section-border highlight">47.92</td>
                                <td>28.67</td>
                                <td>57.25</td>
                                <td>57.66</td>
                                <td>55.92</td>
                            </tr>
                            </tbody>
                        </table>
                    </div>
                    <figcaption style="text-align: center; width: 140%;">
                        Table 2: All Benchmark Results for Model Ensemble with 1.2M Adapter Data + 737K
                        Instruction Tuning Data
                    </figcaption>
                </div>

                <p class="text">
                    However, this strategy has two limitations:
                    1) it employs interpolation, which can potentially lead to information loss, especially on vision encoders with high-resolution feature maps, and
                    2) it treats each model equally by simple concatenation.
                    Therefore, we seek a more effective strategy that fully leverages model combinations with less information loss and more flexibility.
                </p>

        </div>
        </div>

        <div id='connector_design' class="connector-block">

            <h1 class="text">Spatial Vision Aggregator (SVA): A New Connector Design</h1>
            <p class="text">
                To effectively aggregate features from multiple vision encoders and reduce information loss during interpolation, we use a set of learnable latent queries that interact with multiple vision features through cross-attention layers<d-cite key="dai2024instructblip"></d-cite>.
                In particular, our approach incorporates two new vision-centric design principles:
                <ol class="text">
                    <li>We encode spatial inductive bias by explicitly localizing the aggregation space for each token in the query.</li>
                    <li>We perform vision feature aggregation multiple times across the LLM layers, allowing the model to repeatedly reference necessary visual information.</li>
                </ol>
            </p>
            <d-figure id="fig-vision_connector">
                <figure>
                    <img data-zoomable="" draggable="false" src="static/img/sva.png" alt="Spatial Vision Aggregator (SVA)">
                    <figcaption>
                        <strong>Figure 6:</strong> Spatial Vision Aggregator (SVA).
                    </figcaption>
                </figure>
            </d-figure>
        </div>

        <div id="instruction_data" class="data-block">
            <h1 class="text">Instruction Tuning Data for Training MLLMs</h1>
            <p class="text">
                Previous work highlights the importance of data in training MLLMs, but explicit investigations are limited.
                In this study, we gather all available instruction tuning data and examine data curation by enhancing diversity, balancing sources, and improving mixtures.

            </p>

            <div class="subsection">
                <h3 class="text">Data Collection</h3>
                <p class="text" id="data_collection">
                    <strong>Collecting Instruction Tuning Data from existing data sources</strong>
                    We first use existing multimodal benchmarks and datasets involving visual interaction data,
                    such as Visual Question Answering (VQA) and OCR data.
                    We also collect a small volume of high-quality language-only instruction-following data to maintain its language ability.
                </p>
                <d-figure id="fig-cambrian7m">
                    <figure>
                        <img data-zoomable="" draggable="false" src="static/img/cambrian_7m.png" alt="Cambrian-7M: A Large-Scale Curated Instruction Tuning Dataset for Training MLLM">
                        <figcaption>
                            <strong>Figure 7:</strong> Cambrian-7M: A Large-Scale Curated Instruction Tuning Dataset for Training MLLM.
                        </figcaption>
                    </figure>
                </d-figure>
                <p class="text">
                    <strong>Targeted Internet Data Collection Engine</strong>
                    We also introduce a data engine designed to create large-scale, reliable,
                    high-quality knowledge-based multimodal instruction tuning data.
                </p>
                <d-figure id="fig-dataengine">
                    <figure>
                        <img data-zoomable="" draggable="false" src="static/img/dataenginefigurepdf_crop.png" alt="Targeted Internet Data Collection Engine">
                        <figcaption>
                            <strong>Figure 8:</strong> Targeted Internet Data Collection Engine.
                        </figcaption>
                    </figure>
                </d-figure>

                <p class="text">
                    <strong>Cambrian-10M</strong>
                    To this end, we create a large pool of instruction tuning data, which we refer to as Cambrian-10M.
                    This pool contains approximately 9784k data points, offering a diverse range of data for our work and future research.
                    We visualize its composition in <a href="#fig-cambrian7m">Figure 7</a>.
                </p>
            </div>

            <div id="sec:data_curation" class="subsection">
                <h3 class="text">Data Curation</h3>
                <p class="text">
                    Cambrian-10M is a large pool of instruction tuning data sourced from a variety of data sources,
                    with an unbalanced data ratio between categories.
                    Here, we take a preliminary step to study data curation by improving data balancing and adjusting data ratios.
                </p>

                <p class="text" id="data_curation">
                    <strong>Data Balancing</strong>
                    We follow previous work to set thresholds t
                    for the number of data points from a single data source.
                    We choose t = 150k, 250k, 350k, and 450k in this section and observe an
                    elbow effect in <a href="#tab:data_balance_result">Table 3</a>&mdash;finding that a threshold between 250k and 350k work the best for Cambrian-10M.
                </p>
                <d-figure id="fig-filter_k">
                    <figure>
                        <img data-zoomable="" draggable="false" src="static/img/Cumulative_Sum_of_Counts.png" alt="Data Balancing via Applying Thresholds on Data Sources">
                        <figcaption>
                            <strong>Figure 9:</strong> Data Balancing via Applying Thresholds on Data Sources.
                        </figcaption>
                    </figure>
                </d-figure>
                <br>
                <div id="tab:data_balance_result" style="display: flex; flex-direction: column; align-items: center;">
                    <div class="table-container">
                        <table class="data-table">
                          <thead>
                            <tr>
                              <th></th>
                              <th>Average</th>
                              <th>General</th>
                              <th>Knowledge</th>
                              <th>OCR & Chart</th>
                              <th>Vision-Centric</th>
                            </tr>
                          </thead>
                          <tbody>
                            <tr>
                              <td>150k</td>
                              <td>53.7</td>
                              <td>68.0</td>
                              <td>51.3</td>
                              <td>45.2</td>
                              <td>50.5</td>
                            </tr>
                            <tr>
                              <td>250k</td>
                              <td class="highlight">54.3</td>
                              <td class="highlight">68.1</td>
                              <td>51.5</td>
                              <td>45.3</td>
                              <td>52.2</td>
                            </tr>
                            <tr>
                              <td>350k</td>
                              <td class="highlight">54.3</td>
                              <td>67.4</td>
                              <td>51.4</td>
                              <td class="highlight">46.0</td>
                              <td class="highlight">52.3</td>
                            </tr>
                            <tr>
                              <td>450k</td>
                              <td>54.2</td>
                              <td>68.0</td>
                              <td class="highlight">52.2</td>
                              <td>45.5</td>
                              <td>50.7</td>
                            </tr>
                          </tbody>
                        </table>
                      </div>

                    <figcaption style="text-align: center; width: 100%;">
                        <strong>Table 3:</strong> Threshold 𝑡 value between 250k and 350k obtains better performance.
                    </figcaption>
                </div>

                <p class="text">
                    <strong>Data Ratio</strong>
                    Given the various capabilities of different types of visual instruction tuning data, it is essential to balance the ratio of these data types.
                    We conduct pilot experiments with a fixed dataset size of 1350k,
                    examining the impact of different data ratios on downstream performance.
                    We visualize the results in <a href="#fig-data_ratio">Figure 10</a> and summarize our findings as follows:
                    (i) Balancing General, OCR, and Language data is crucial.
                    (ii) Performance on knowledge-intensive tasks is influenced by multiple factors,
                    often requiring a mix of OCR, chart, reasoning, and general perception.
                </p>
                <d-figure id="fig-data_ratio">
                    <figure>
                        <img data-zoomable="" draggable="false" src="static/img/data_mixture_ratio_w_avg_score.png" alt="Exploring instruction tuning data mixture ratios">
                        <figcaption>
                            <strong>Figrue 10:</strong> Exploring instruction tuning data mixture ratios.
                        </figcaption>
                    </figure>
                </d-figure>

                <p class="text">
                    <strong>Cambrian-7M</strong>
                    By applying data filtering to Cambrian-10M with our identified data ratio, we create a smaller but higher-quality dataset called Cambrian-7M.
                    <a href="#tab:data_ratio_result">Table 4</a> showcases the benefits of a well-balanced and carefully curated dataset. Despite having fewer samples, Cambrian-7M demonstrates improved performance.
                </p>
                <div id="tab:data_ratio_result" style="display: flex; flex-direction: column; align-items: center;">
                    <div class="table-container">
                        <table class="data-table">
                          <thead>
                            <tr>
                              <th></th>
                              <th>Average</th>
                              <th>General</th>
                              <th>Knowledge</th>
                              <th>OCR & Chart</th>
                              <th>Vision-Centric</th>
                            </tr>
                          </thead>
                          <tbody>
                            <tr>
                              <td>LLaVA-665K</td>
                              <td>40.7</td>
                              <td>64.7</td>
                              <td>45.2</td>
                              <td>20.8</td>
                              <td>32.0</td>
                            </tr>
                            <tr>
                              <td>Cambrian-10M</td>
                              <td>54.8</td>
                              <td>68.7</td>
                              <td>51.6</td>
                              <td class="highlight">47.3</td>
                              <td>51.4</td>
                            </tr>
                            <tr>
                              <td>Cambrian-7M</td>
                              <td class="highlight">55.9</td>
                              <td class="highlight">69.6</td>
                              <td class="highlight">52.6</td>
                              <td class="highlight">47.3</td>
                              <td class="highlight">54.1</td>
                            </tr>
                          </tbody>
                        </table>
                      </div>
                    <figcaption style="text-align: center; width: 100%;">
                        Table 4: Performance improves with better instruction tuning data curation.
                    </figcaption>
                </div>


            </div>

            <div class="subsection">
                <h3 class="text">Alleviating the "Answer Machine Phenomenon" via System Prompts</h3>
                <p class="text">
                    Here, we investigate a phenomenon we term the "answer machine phenomenon."
                    We observe that a well-trained MLLM may excel at VQA benchmarks, but lack basic conversational abilities and default to outputting short, curt responses (see examples in <a href="#fig-sysprompt">Figure 5</a>).
                </p>

                <p class="text">
                    To address this, we find that incorporating additional system prompts during training mitigates this phenomenon.
                    We append prompts such as "<em>Answer the question using a single word or phrase.</em>"
                    before questions that generate a single word or phrase in the response.
                    We observe that after integrating these system prompts, the model's benchmark performance remains unchanged,
                    while its conversational ability significantly improves.
                </p>
                <d-figure id="fig-sysprompt">
                    <figure>
                        <img data-zoomable="" draggable="false" src="static/img/sysprompt.jpg" alt="Incorporating System Prompt in Instruction Tuning Data alleviates "Answer Machine Phenomenon">
                        <figcaption>
                            <strong>Figure 11:</strong> Incorporating System Prompt in Instruction Tuning Data alleviates the "Answer Machine Phenomenon".
                        </figcaption>
                    </figure>
                </d-figure>
            </div>
        </div>

        <div id='sota' class="sota-block">
            <h1 class="text">State of the Art MLLM Performance</h1>
            <p class="text">
                Finally, we leverage the insights from all of our previous studies to train a high-performance Cambrian model.
                We train with three different sizes of LLM backbones: LLaMA-3-Instruct-8B, Vicuna-1.5-13B, and Hermes-2-Yi-34B.
                Our visual tower uses a combination of four models&mdash;SigLIP, CLIP, DINOv2, and OpenCLIP ConvNeXt
                (see <a href="#sec:model_ensemble">Combining Multiple Vision Encoders</a>) with the <a href="#connector_design">Spatial Vision Aggregator</a>.
                We use 2.5M adapter data and Cambrian-7M instruction tuning data (see <a href="#sec:data_curation">Data Curation</a>).
                We evaluate our models on the <a href="#sec:benchmarking">categorized benchmarks</a>, and tabulate the results in <a href="#tab:final_table">Table 5</a>. Cambrian-1 exceeds other open-source models such as LLaVA-NeXT and Mini-Gemini, and achieves comparable performance on a number of benchmarks with the best proprietary models such as GPT-4V, Gemini-Pro, and MM-1.
            </p>
            <div id="tab:final_table" style="display: flex; flex-direction: column; align-items: center;" class="figure">
                <div class="table-container">
                    <table class="data-table">
                        <thead>
                        <tr>
                            <th colspan="2" class="tb-hdr">Model</th>
                            <th colspan="5" class="tb-hdr">General</th>
                            <th colspan="5" class="tb-hdr">Knowledge</th>
                            <th colspan="5" class="tb-hdr">OCR & Chart</th>
                            <th colspan="5" class="tb-hdr">Vision-Centric</th>
                        </tr>
                        <tr>
                            <th>Method</th>
                            <th class="rotate"># Vis Tok.</th>
                            <th class="rotate">Avg</th>
                            <th class="rotate">MME<sup>P</sup></th>
                            <th class="rotate">MMB</th>
                            <th class="rotate">SEED<sup>I</sup></th>
                            <th class="rotate">GQA</th>
                            <th class="rotate">Avg</th>
                            <th class="rotate">SQA<sup>I</sup></th>
                            <th class="rotate">MMMU<sup>V</sup></th>
                            <th class="rotate">MathVista<sup>M</sup></th>
                            <th class="rotate">AI2D</th>
                            <th class="rotate">Avg</th>
                            <th class="rotate">ChartQA</th>
                            <th class="rotate">OCRBench</th>
                            <th class="rotate">TextVQA</th>
                            <th class="rotate">DocVQA</th>
                            <th class="rotate">Avg</th>
                            <th class="rotate">MMVP</th>
                            <th class="rotate">RealworldQA</th>
                            <th class="rotate">CV-Bench<sup>2D</sup></th>
                            <th class="rotate">CV-Bench<sup>3D</sup></th>
                        </tr>
                        </thead>
                        <tbody>
                        <tr>
                            <td>GPT-4V</td>
                            <td>UNK.</td>
                            <td>63.0</td>
                            <td>1409.4</td>
                            <td>75.8</td>
                            <td>69.1</td>
                            <td>36.8</td>
                            <td>65.2</td>
                            <td>75.7</td>
                            <td>56.8</td>
                            <td>49.9</td>
                            <td>78.2</td>
                            <td>77.4</td>
                            <td>78.5</td>
                            <td>64.5</td>
                            <td>78.0</td>
                            <td>88.4</td>
                            <td>62.4</td>
                            <td>50.0</td>
                            <td>61.4</td>
                            <td>64.3</td>
                            <td>73.8</td>
                        </tr>
                        <tr>
                            <td>Gemini-1.0 Pro</td>
                            <td>UNK.</td>
                            <td>-</td>
                            <td>1496.6</td>
                            <td>73.6</td>
                            <td>70.7</td>
                            <td>-</td>
                            <td>-</td>
                            <td>79.5</td>
                            <td>47.9</td>
                            <td>45.2</td>
                            <td>-</td>
                            <td>-</td>
                            <td>-</td>
                            <td>65.9</td>
                            <td>-</td>
                            <td>-</td>
                            <td>-</td>
                            <td>-</td>
                            <td>-</td>
                            <td>-</td>
                            <td>-</td>
                        </tr>
                        <tr>
                            <td>Gemini-1.5 Pro</td>
                            <td>UNK.</td>
                            <td>-</td>
                            <td>-</td>
                            <td>-</td>
                            <td>-</td>
                            <td>-</td>
                            <td>-</td>
                            <td>-</td>
                            <td>58.5</td>
                            <td>52.1</td>
                            <td>80.3</td>
                            <td>-</td>
                            <td>81.3</td>
                            <td>-</td>
                            <td>73.5</td>
                            <td>86.5</td>
                            <td>-</td>
                            <td>-</td>
                            <td>67.5</td>
                            <td>-</td>
                            <td>-</td>
                        </tr>
                        <tr>
                            <td>Grok-1.5</td>
                            <td>UNK.</td>
                            <td>-</td>
                            <td>-</td>
                            <td>-</td>
                            <td>-</td>
                            <td>-</td>
                            <td>-</td>
                            <td>-</td>
                            <td>53.6</td>
                            <td>52.8</td>
                            <td>88.3</td>
                            <td>-</td>
                            <td>76.1</td>
                            <td>-</td>
                            <td>78.1</td>
                            <td>85.6</td>
                            <td>-</td>
                            <td>-</td>
                            <td>68.7</td>
                            <td>-</td>
                            <td>-</td>
                        </tr>
                        <tr>
                            <td>MM-1-8B</td>
                            <td>144</td>
                            <td>-</td>
                            <td>1529.3</td>
                            <td>72.3</td>
                            <td>69.9</td>
                            <td>-</td>
                            <td>-</td>
                            <td>72.6</td>
                            <td>37.0</td>
                            <td>35.9</td>
                            <td>-</td>
                            <td>-</td>
                            <td>-</td>
                            <td>-</td>
                            <td>-</td>
                            <td>-</td>
                            <td>-</td>
                            <td>-</td>
                            <td>-</td>
                            <td>-</td>
                            <td>-</td>
                        </tr>
                        <tr>
                            <td>MM-1-30B</td>
                            <td>144</td>
                            <td>-</td>
                            <td>1637.6</td>
                            <td>75.1</td>
                            <td>72.1</td>
                            <td>-</td>
                            <td>-</td>
                            <td>81.0</td>
                            <td>44.7</td>
                            <td>39.4</td>
                            <td>-</td>
                            <td>-</td>
                            <td>-</td>
                            <td>-</td>
                            <td>-</td>
                            <td>-</td>
                            <td>-</td>
                            <td>-</td>
                            <td>-</td>
                            <td>-</td>
                            <td>-</td>
                        </tr>
                        <tr class="highlight-gray">
                            <td colspan="22"><i>Base LLM: Llama-3-Ins-8B</i></td>
                        </tr>
                        <tr>
                            <td>Mini-Gemini-HD-8B</td>
                            <td>2880</td>
                            <td>72.7</td>
                            <td><b>1606.0</b></td>
                            <td>72.7</td>
                            <td>73.2</td>
                            <td>64.5</td>
                            <td>55.7</td>
                            <td>75.1</td>
                            <td>37.3</td>
                            <td>37.0</td>
                            <td><b>73.5</b></td>
                            <td>62.9</td>
                            <td>59.1</td>
                            <td>47.7</td>
                            <td>70.2</td>
                            <td>74.6</td>
                            <td>51.5</td>
                            <td>18.7</td>
                            <td>62.1</td>
                            <td>62.2</td>
                            <td>63.0</td>
                        </tr>
                        <tr>
                            <td>LLaVA-NeXT-8B</td>
                            <td>2880</td>
                            <td>72.5</td>
                            <td>1603.7</td>
                            <td>72.1</td>
                            <td>72.7</td>
                            <td><b>65.2</b></td>
                            <td>55.6</td>
                            <td>72.8</td>
                            <td>41.7</td>
                            <td>36.3</td>
                            <td>71.6</td>
                            <td>63.9</td>
                            <td>69.5</td>
                            <td>49.0</td>
                            <td>64.6</td>
                            <td>72.6</td>
                            <td>56.6</td>
                            <td>38.7</td>
                            <td>60.1</td>
                            <td>62.2</td>
                            <td>65.3</td>
                        </tr>
                        <tr class="highlight-orange">
                            <td>Cambrian-1-8B</td>
                            <td class="highlight-blue">576</td>
                            <td><b>73.1</b></td>
                            <td>1,547.1</td>
                            <td><b>75.9</b></td>
                            <td><b>74.7</b></td>
                            <td>64.6</td>
                            <td><b>61.3</b></td>
                            <td><b>80.4</b></td>
                            <td><b>42.7</b></td>
                            <td><b>49.0</b></td>
                            <td>73.0</td>
                            <td><b>71.3</b></td>
                            <td><b>73.3</b></td>
                            <td><b>62.4</b></td>
                            <td><b>71.7</b></td>
                            <td><b>77.8</b></td>
                            <td><b>65.0</b></td>
                            <td><b>51.3</b></td>
                            <td><b>64.2</b></td>
                            <td><b>72.3</b></td>
                            <td><b>72.0</b></td>
                        </tr>
                        <tr class="highlight-gray">
                            <td colspan="22"><i>Base LLM: Vicuna-1.5-13B</i></td>
                        </tr>
                        <tr>
                            <td>Mini-Gemini-HD-13B</td>
                            <td>2880</td>
                            <td>70.7</td>
                            <td>1597.0</td>
                            <td>68.6</td>
                            <td>70.6</td>
                            <td>63.7</td>
                            <td>54.1</td>
                            <td>71.9</td>
                            <td>37.3</td>
                            <td>37.0</td>
                            <td>70.1</td>
                            <td>60.8</td>
                            <td>56.6</td>
                            <td>46.6</td>
                            <td>70.2</td>
                            <td>69.8</td>
                            <td>49.4</td>
                            <td>19.3</td>
                            <td>57.5</td>
                            <td>53.6</td>
                            <td>67.3</td>
                        </tr>
                        <tr>
                            <td>LLaVA-NeXT-13B</td>
                            <td>2880</td>
                            <td>69.9</td>
                            <td>1575.0</td>
                            <td>70.0</td>
                            <td>65.6</td>
                            <td><b>65.4</b></td>
                            <td>53.7</td>
                            <td>73.5</td>
                            <td>36.2</td>
                            <td>35.1</td>
                            <td>70.0</td>
                            <td>62.9</td>
                            <td>62.2</td>
                            <td>51.4</td>
                            <td>67.1</td>
                            <td>70.9</td>
                            <td>55.9</td>
                            <td>36.0</td>
                            <td>59.1</td>
                            <td>62.7</td>
                            <td>65.7</td>
                        </tr>
                        <tr class="highlight-orange">
                            <td>Cambrian-1-13B</td>
                            <td class="highlight-blue">576</td>
                            <td><b>73.7</b></td>
                            <td><b>1,610.4</b></td>
                            <td><b>75.7</b></td>
                            <td><b>74.4</b></td>
                            <td>64.3</td>
                            <td><b>60.2</b></td>
                            <td><b>79.3</b></td>
                            <td><b>40.0</b></td>
                            <td><b>48.0</b></td>
                            <td><b>73.6</b></td>
                            <td><b>71.3</b></td>
                            <td><b>73.8</b></td>
                            <td><b>61.9</b></td>
                            <td><b>72.8</b></td>
                            <td><b>76.8</b></td>
                            <td><b>62.2</b></td>
                            <td><b>41.3</b></td>
                            <td><b>63.0</b></td>
                            <td><b>72.5</b></td>
                            <td><b>71.8</b></td>
                        </tr>
                        <tr class="highlight-gray">
                            <td colspan="22"><i>Base LLM: Hermes2-Yi-34B</i></td>
                        </tr>
                        <tr>
                            <td>Mini-Gemini-HD-34B</td>
                            <td>2880</td>
                            <td>76.2</td>
                            <td>1659.0</td>
                            <td>80.6</td>
                            <td>75.3</td>
                            <td>65.8</td>
                            <td>62.4</td>
                            <td>77.7</td>
                            <td>48.0</td>
                            <td>43.4</td>
                            <td><b>80.5</b></td>
                            <td>68.1</td>
                            <td>67.6</td>
                            <td>51.8</td>
                            <td>74.1</td>
                            <td><b>78.9</b></td>
                            <td>63.8</td>
                            <td>37.3</td>
                            <td>67.2</td>
                            <td>71.5</td>
                            <td>79.2</td>
                        </tr>
                        <tr>
                            <td>LLaVA-NeXT-34B</td>
                            <td>2880</td>
                            <td>76.0</td>
                            <td>1633.2</td>
                            <td>79.3</td>
                            <td><b>75.9</b></td>
                            <td><b>67.1</b></td>
                            <td>62.5</td>
                            <td>81.8</td>
                            <td>46.7</td>
                            <td>46.5</td>
                            <td>74.9</td>
                            <td>67.7</td>
                            <td>68.7</td>
                            <td>54.5</td>
                            <td>69.5</td>
                            <td>78.1</td>
                            <td>64.0</td>
                            <td>47.3</td>
                            <td>61.0</td>
                            <td>73.0</td>
                            <td>74.8</td>
                        </tr>
                        <tr class="highlight-orange">
                            <td>Cambrian-1-34B</td>
                            <td class="highlight-blue">576</td>
                            <td><b>76.8</b></td>
                            <td><b>1689.3</b></td>
                            <td><b>81.4</b></td>
                            <td>75.3</td>
                            <td>65.8</td>
                            <td><b>67.0</b></td>
                            <td><b>85.6</b></td>
                            <td><b>49.7</b></td>
                            <td><b>53.2</b></td>
                            <td>79.7</td>
                            <td><b>71.9</b></td>
                            <td><b>75.6</b></td>
                            <td><b>60.0</b></td>
                            <td><b>76.7</b></td>
                            <td>75.5</td>
                            <td><b>68.5</b></td>
                            <td><b>52.7</b></td>
                            <td><b>67.8</b></td>
                            <td><b>74.0</b></td>
                            <td><b>79.7</b></td>
                        </tr>
                        </tbody>
                    </table>
                </div>
                <figcaption style="text-align: center; width: 140%;">
                    Table 5: Cambrian-1 outperforms other open-source models and achieves comparable performance with proprietary models, while using only 576 visual tokens.
                </figcaption>
            </div>
            <p style="padding: 1em"></p>
            <d-figure id="fig-comparison">
                <figure>
                    <img data-zoomable="" draggable="false" src="static/img/comparison.PNG" alt="Cambrian-1 outperforms other open-source models and achieves comparable performance with proprietary models">
                    <figcaption>
                        <strong>Figure 12:</strong> Cambrian-1 outperforms other open-source models and achieves comparable performance with proprietary models.
                    </figcaption>
                </figure>
            </d-figure>

        </div>

        <div id="conclusion" style="position: relative; margin-top: 40px; margin-bottom: 0px;">
            <h2 class="text" style="margin-top:0px; margin-bottom:10px">Conclusion</h2>
            <p class="text">
                To conclude, Cambrian-1 is a family of state-of-the-art MLLMs that achieve top performance across diverse benchmarks
                and excel in visual-centric tasks. We provide model weights, open-source code, datasets, and detailed recipes for model training and evaluation.
                We hope our work will strengthen the open research community and accelerate research in both visual representation learning and multimodal systems.
            </p>
        </div>

        </d-article>
        <d-appendix>
            <h3>BibTeX</h3>
            <p class="bibtex">
                @article{liu2025ir3d,<br>
                &nbsp;&nbsp;title={IR3D-Bench: Evaluating Vision-Language Model Scene Understanding as Agentic Inverse Rendering},<br>
                &nbsp;&nbsp;author={Liu, Hengyu and Li, Chenxin and Li, Zhengxin and Wu, Yipeng, and Li, Wuyang and Yang, Zhiqin and Zhang, Zhenyuan and Lin, Yunlong and Han, Sirui and Feng, Brandon},<br>
                &nbsp;&nbsp;journal={arXiv preprint},<br>
                &nbsp;&nbsp;year={2025}<br>
                }
            </p>

            <d-footnote-list></d-footnote-list>
            <d-citation-list></d-citation-list>
        </d-appendix>
        <!-- bibliography will be inlined during Distill pipeline's pre-rendering -->
        <script src="./static/js/nav-bar.js"></script>
    </body>
</html>
